{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pycolmap\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import h5py as h5\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "from hloc.utils.io import list_h5_names, get_matches, get_keypoints, find_pair\n",
    "from hloc.visualization import plot_images, plot_keypoints, plot_matches, read_image, add_text, cm_RdGn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR = \"../image-matching-challenge-2023\"\n",
    "MODE = \"train\"\n",
    "NAME1 = \"SP+LG-rot\"\n",
    "NAME2 = \"DISK+LG-rot\"\n",
    "dataset = \"heritage\" # \"heritage\", \"haiper\", \"urban\"\n",
    "scene = \"cyprus\" # \"dioscuri\", \"cyprus\", \"wall\", \"kyiv-puppet-theater\", \"bike\", \"chairs\", \"fountain\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features1_dir = f\"../outputs/{NAME1}/{dataset}/{scene}/features.h5\"\n",
    "features2_dir = f\"../outputs/{NAME2}/{dataset}/{scene}/features.h5\"\n",
    "\n",
    "matches1_dir = f\"../outputs/{NAME1}/{dataset}/{scene}/matches.h5\"\n",
    "matches2_dir = f\"../outputs/{NAME2}/{dataset}/{scene}/matches.h5\"\n",
    "\n",
    "images = Path(os.path.join(DIR, MODE, dataset, scene, \"images\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features1 = h5.File(features1_dir, \"r\")\n",
    "features2 = h5.File(features2_dir, \"r\")\n",
    "\n",
    "matches1_path = h5.File(matches1_dir, \"r\")\n",
    "matches2_path = h5.File(matches2_dir, \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_names = os.listdir(images)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenate Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_features(features1: Path, features2: Path, out_path: Path):\n",
    "    # read features\n",
    "    img_list = list_h5_names(features1) + list_h5_names(features2)\n",
    "    img_list = list(set(img_list))\n",
    "    ensemble_features = {}\n",
    "    with h5.File(features1, \"r\") as f1:\n",
    "        with h5.File(features2, \"r\") as f2:\n",
    "            for img in tqdm(img_list, desc=\"concatenating features\", ncols=80):\n",
    "                kpts1  = f1[img][\"keypoints\"] if img in f1.keys() else np.array([])\n",
    "                kpts2  = f2[img][\"keypoints\"] if img in f2.keys() else np.array([])\n",
    "\n",
    "                scores1 = f1[img][\"scores\"] if img in f1.keys() else np.array([])\n",
    "                scores2 = f2[img][\"scores\"] if img in f2.keys() else np.array([])\n",
    "\n",
    "                n_feats1 = len(kpts1) if img in f1.keys() else 0\n",
    "                n_feats2 = len(kpts2) if img in f2.keys() else 0\n",
    "\n",
    "                keypoints = np.concatenate([kpts1, kpts2], axis=0)\n",
    "                scores = np.concatenate([scores1, scores2], axis=0)\n",
    "\n",
    "                ensemble_features[img] = {\n",
    "                    \"keypoints\": keypoints,\n",
    "                    \"scores\": scores,\n",
    "                    \"counts\": [n_feats1, n_feats2]\n",
    "                }\n",
    "\n",
    "    # write features\n",
    "    ens_kp_ds = h5.File(out_path, \"w\")\n",
    "    for img in ensemble_features:\n",
    "\n",
    "        ens_kp_ds.create_group(img)\n",
    "        for k in ensemble_features[img].keys():\n",
    "            ens_kp_ds[img].create_dataset(k, data=ensemble_features[img][k])\n",
    "\n",
    "        if img == \"3DOM_FBK_IMG_1516.png\":\n",
    "            print(ens_kp_ds[img][\"counts\"])\n",
    "\n",
    "    ens_kp_ds.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ens_feat_path = Path(f\"../outputs/ensemble/{dataset}/{scene}/ens_features.h5\")\n",
    "\n",
    "if not ens_feat_path.exists():\n",
    "    ens_feat_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "concat_features(features1_dir, features2_dir, ens_feat_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features 1\n",
    "plot_images([read_image(images / imname) for imname in image_names[:4]])\n",
    "plot_keypoints([get_keypoints(features1_dir, imname) for imname in image_names[:4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features 2\n",
    "plot_images([read_image(images / imname) for imname in image_names[:4]])\n",
    "plot_keypoints([get_keypoints(features2_dir, imname) for imname in image_names[:4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble Features\n",
    "plot_images([read_image(images / imname) for imname in image_names[:4]])\n",
    "plot_keypoints([get_keypoints(ens_feat_path, imname) for imname in image_names[:4]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenate Matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_matches(matches, scores, num_keypoints1, num_keypoints2):\n",
    "    rev_matches = np.ones(num_keypoints2) * -1\n",
    "    rev_scores = np.zeros(num_keypoints2)\n",
    "\n",
    "    assert len(matches) == num_keypoints1, \"Number of matches must equal number of keypoints in image 1\"\n",
    "    assert np.max(matches) < num_keypoints2, \"Matches must be indices of keypoints in image 2\"\n",
    "\n",
    "    # matches is a list of length nkps1 with each value being either -1 or the index of the match in nkps2\n",
    "    for i, m in enumerate(matches):\n",
    "        if m != -1:\n",
    "            rev_matches[m] = i\n",
    "            rev_scores[m] = scores[i]\n",
    "\n",
    "    return rev_matches.astype(int), rev_scores\n",
    "\n",
    "def extract_matches(matches, features, name0, name1, idx=0):\n",
    "    nkpts0 = features[name0][\"counts\"][idx]\n",
    "    nkpts1 = features[name1][\"counts\"][idx]\n",
    "\n",
    "    try:\n",
    "        p, rev = find_pair(matches, name0, name1)\n",
    "    except ValueError:\n",
    "        # print(f\"Could not find pair {name0} - {name1}\")\n",
    "        m = np.ones(nkpts0) * -1\n",
    "        sc = np.zeros(nkpts0)\n",
    "        return m, sc\n",
    "\n",
    "    m = matches[p][\"matches0\"].__array__()\n",
    "    sc = matches[p][\"matching_scores0\"].__array__()\n",
    "\n",
    "    return reverse_matches(m, sc, nkpts1, nkpts0) if rev else (m, sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_matches(matches1_path: Path, matches2_path: Path, ensemble_features_path: Path, out_path: Path):\n",
    "    # concat matches\n",
    "    ensemble_matches = {}\n",
    "    with h5.File(matches1_path, \"r\") as matches1:\n",
    "        with h5.File(matches2_path, \"r\") as matches2:\n",
    "            with h5.File(ensemble_features_path, \"r\") as ensemble_features:\n",
    "                pairs = list_h5_names(matches1_path) + list_h5_names(matches2_path)\n",
    "                pairs = [sorted(p.split(\"/\"))[0] + \"/\" + sorted(p.split(\"/\"))[1] for p in pairs]\n",
    "                pairs = sorted(list(set(pairs)))\n",
    "\n",
    "                print(f\"Found {len(pairs)} pairs\")\n",
    "                print(f\"Pairs in matches1: {len(list_h5_names(matches1_path))}\")\n",
    "                print(f\"Pairs in matches2: {len(list_h5_names(matches2_path))}\")\n",
    "                \n",
    "                for pair in tqdm(pairs, desc=\"concatenating matches\", ncols=80):\n",
    "                    name0, name1 = pair.split(\"/\")\n",
    "\n",
    "                    # prepare dict\n",
    "                    if name0 not in ensemble_matches:\n",
    "                        ensemble_matches[name0] = {}\n",
    "                    if name1 not in ensemble_matches[name0]:\n",
    "                        ensemble_matches[name0][name1] = {}\n",
    "\n",
    "                    # get matches1\n",
    "                    m1, sc1 = extract_matches(matches1, ensemble_features, name0, name1, idx=0)\n",
    "\n",
    "                    # get matches2\n",
    "                    m2, sc2 = extract_matches(matches2, ensemble_features, name0, name1, idx=1)\n",
    "\n",
    "                    # concat matches\n",
    "                    offset = ensemble_features[name1][\"counts\"][0]\n",
    "                    m2 += offset * np.where(m2 != -1, 1, 0)\n",
    "\n",
    "                    ensemble_matches[name0][name1][\"matches0\"] = np.concatenate(\n",
    "                        [m1, m2], axis=0\n",
    "                    )\n",
    "\n",
    "                    ensemble_matches[name0][name1][\"matching_scores0\"] = np.concatenate(\n",
    "                        [sc1, sc2], axis=0\n",
    "                    )\n",
    "\n",
    "    ens_matches_ds = h5.File(out_path, \"w\")\n",
    "    for img1 in ensemble_matches:\n",
    "        ens_matches_ds.create_group(img1)\n",
    "        for img2 in ensemble_matches[img1].keys():\n",
    "            ens_matches_ds[img1].create_group(img2)\n",
    "            for k in ensemble_matches[img1][img2].keys():\n",
    "                ens_matches_ds[img1][img2].create_dataset(\n",
    "                    k, data=ensemble_matches[img1][img2][k]\n",
    "                )\n",
    "\n",
    "    ens_matches_ds.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ens_matches_path = Path(f\"../outputs/ensemble/{dataset}/{scene}/ens_matches.h5\")\n",
    "concat_matches(matches1_dir, matches2_dir, ens_feat_path, ens_matches_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = sorted(list_h5_names(ens_matches_path))\n",
    "len(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = sorted(list_h5_names(matches1_dir))\n",
    "match_matrix1 = -np.ones([len(image_names), len(image_names)])\n",
    "for pair in pairs:\n",
    "    name0, name1 = pair.split(\"/\")\n",
    "    idx0, idx1 = image_names.index(name0), image_names.index(name1)\n",
    "    m, sc = get_matches(matches1_dir, name0, name1)\n",
    "    match_matrix1[idx0, idx1] = match_matrix1[idx1, idx0] = m.shape[0]\n",
    "\n",
    "ax = sns.heatmap(match_matrix1, linewidth=0.0, cmap=\"hot\", mask=match_matrix1 < 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = sorted(list_h5_names(matches2_dir))\n",
    "match_matrix2 = -np.ones([len(image_names), len(image_names)])\n",
    "for pair in pairs:\n",
    "    name0, name1 = pair.split(\"/\")\n",
    "    idx0, idx1 = image_names.index(name0), image_names.index(name1)\n",
    "    m, sc = get_matches(matches2_dir, name0, name1)\n",
    "    match_matrix2[idx0, idx1] = match_matrix2[idx1, idx0] = m.shape[0]\n",
    "\n",
    "ax = sns.heatmap(match_matrix2, linewidth=0.0, cmap=\"hot\", mask=match_matrix2 < 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = sorted(list_h5_names(matches1_dir))\n",
    "match_matrix_ens = -np.ones([len(image_names), len(image_names)])\n",
    "for pair in pairs:\n",
    "    name0, name1 = pair.split(\"/\")\n",
    "    idx0, idx1 = image_names.index(name0), image_names.index(name1)\n",
    "    m, sc = get_matches(ens_matches_path, name0, name1)\n",
    "    match_matrix_ens[idx0, idx1] = match_matrix_ens[idx1, idx0] = m.shape[0]\n",
    "\n",
    "ax = sns.heatmap(match_matrix_ens, linewidth=0.0, cmap=\"hot\", mask=match_matrix_ens < 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if scene == \"cyprus\":\n",
    "    name1 = image_names[7]\n",
    "    name2 = image_names[23]\n",
    "else:\n",
    "    name1 = image_names[0]\n",
    "    name2 = image_names[5]\n",
    "\n",
    "plot_images([read_image(images / name1), read_image(images / name2)])\n",
    "kp0, kp1 = get_keypoints(features1_dir, name1), get_keypoints(features1_dir, name2)\n",
    "m, sc = get_matches(matches1_dir, name1, name2)\n",
    "\n",
    "plot_keypoints([kp0, kp1])\n",
    "plot_matches(kp0[m[:,0]], kp1[m[:,1]])\n",
    "add_text(0, f\"Matches: {m.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images([read_image(images / name1), read_image(images / name2)])\n",
    "kp0, kp1 = get_keypoints(features2_dir, name1), get_keypoints(features2_dir, name2)\n",
    "m, sc = get_matches(matches2_dir, name1, name2)\n",
    "\n",
    "plot_keypoints([kp0, kp1])\n",
    "plot_matches(kp0[m[:,0]], kp1[m[:,1]])\n",
    "add_text(0, f\"Matches: {m.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images([read_image(images / name1), read_image(images / name2)])\n",
    "kp0, kp1 = get_keypoints(ens_feat_path, name1), get_keypoints(ens_feat_path, name2)\n",
    "m, sc = get_matches(ens_matches_path, name1, name2)\n",
    "\n",
    "plot_keypoints([kp0, kp1])\n",
    "plot_matches(kp0[m[:,0]], kp1[m[:,1]])\n",
    "add_text(0, f\"Matches: {m.shape[0]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SfM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = sorted(list_h5_names(ens_matches_path))\n",
    "\n",
    "pairs_path = Path(f\"../outputs/ensemble/{dataset}/{scene}/pairs.txt\")\n",
    "if not pairs_path.parent.exists():\n",
    "    pairs_path.parent.mkdir(parents=True)\n",
    "    \n",
    "with open(pairs_path, \"w\") as f:\n",
    "    for pair in pairs:\n",
    "        p = pair.split(\"/\")\n",
    "        f.write(f\"{p[0]} {p[1]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hloc import reconstruction\n",
    "\n",
    "sfm_dir = Path(f\"../outputs/ensemble/{dataset}/{scene}/sparse\")\n",
    "sfm_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "pairs_path = Path(f\"../outputs/{NAME1}/{dataset}/{scene}/pairs.txt\")\n",
    "\n",
    "feature_path = Path(f\"../outputs/{NAME1}/{dataset}/{scene}/features.h5\")\n",
    "match_path = Path(f\"../outputs/{NAME1}/{dataset}/{scene}/matches.h5\")\n",
    "\n",
    "reconstruction.main(\n",
    "    sfm_dir=sfm_dir,\n",
    "    image_dir=images,\n",
    "    pairs=pairs_path,\n",
    "    features=feature_path,\n",
    "    matches=match_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hloc import reconstruction\n",
    "\n",
    "sfm_dir = Path(f\"../outputs/ensemble/{dataset}/{scene}/sparse\")\n",
    "sfm_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "pairs_path = Path(f\"../outputs/{NAME2}/{dataset}/{scene}/pairs.txt\")\n",
    "\n",
    "feature_path = Path(f\"../outputs/{NAME2}/{dataset}/{scene}/features.h5\")\n",
    "match_path = Path(f\"../outputs/{NAME2}/{dataset}/{scene}/matches.h5\")\n",
    "\n",
    "reconstruction.main(\n",
    "    sfm_dir=sfm_dir,\n",
    "    image_dir=images,\n",
    "    pairs=pairs_path,\n",
    "    features=feature_path,\n",
    "    matches=match_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hloc import reconstruction\n",
    "\n",
    "sfm_dir = Path(f\"../outputs/ensemble/{dataset}/{scene}/sparse\")\n",
    "sfm_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "pairs_path = Path(f\"../outputs/ensemble/{dataset}/{scene}/pairs.txt\")\n",
    "\n",
    "feature_path = Path(f\"../outputs/ensemble/{dataset}/{scene}/ens_features.h5\")\n",
    "match_path = Path(f\"../outputs/ensemble/{dataset}/{scene}/ens_matches.h5\")\n",
    "\n",
    "reconstruction.main(\n",
    "    sfm_dir=sfm_dir,\n",
    "    image_dir=images,\n",
    "    pairs=pairs_path,\n",
    "    features=feature_path,\n",
    "    matches=match_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.rmtree(\"../outputs/ensemble\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "megadepth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
